### 복습 (함수, lamda, 재귀함수)

---

**Q. 람다 함수랑 일반 함수의 성능 차이가 있을까?**

![image.png](attachment:2d59be47-fd26-48d8-a12d-6f35d9300db8:image.png)

**Q. 왜 성능 차이가 없을까? / 둘의 동작 방식의 차이가 있을까?**

→ 핵심 이유: 둘 다 "함수 객체"일 뿐이다

- 파이썬에서는 함수도 일급 객체(first-class object)예요. 즉, 변수에 할당하거나 인자로 넘길 수 있는 **객체**라는 뜻이죠.
- `lambda`로 만든 함수나 `def`로 만든 함수나, 결국 **`function` 객체**로 컴파일됩니다.
- 실행 시점에서 파이썬 인터프리터는 둘을 **동일한 방식으로 처리**해요.
- Copilot의 설명
    
    좋은 질문이에요! 파이썬에서 람다 함수(`lambda`)와 일반 함수(`def`로 정의한 함수) 사이에 **성능 차이가 거의 없는 이유**는, 둘 다 **동일한 방식으로 함수 객체를 생성**하고 실행되기 때문이에요.
    
    🧠 핵심 이유: 둘 다 "함수 객체"일 뿐이다
    
    - 파이썬에서는 함수도 일급 객체(first-class object)예요. 즉, 변수에 할당하거나 인자로 넘길 수 있는 **객체**라는 뜻이죠.
    - `lambda`로 만든 함수나 `def`로 만든 함수나, 결국 **`function` 객체**로 컴파일됩니다.
    - 실행 시점에서 파이썬 인터프리터는 둘을 **동일한 방식으로 처리**해요.
    
    🔍 예시로 비교해볼게요
    
    ```python
    # 일반 함수
    def add(x, y):
        return x + y
    
    # 람다 함수
    add_lambda = lambda x, y: x + y
    
    print(add(3, 4))         # 7
    print(add_lambda(3, 4))  # 7
    ```
    
    이 둘은 내부적으로 거의 동일한 바이트코드로 변환됩니다. `dis` 모듈로 확인해볼 수도 있어요:
    
    ```python
    import dis
    
    dis.dis(add)
    dis.dis(add_lambda)
    ```
    
    ⚖️ 성능 차이가 없는 이유 요약
    
    | 항목 | 설명 |
    | --- | --- |
    | 실행 방식 | 둘 다 동일한 방식으로 호출됨 (`CALL_FUNCTION`) |
    | 내부 구조 | 둘 다 `function` 객체로 생성됨 |
    | 최적화 여부 | 파이썬은 람다라고 해서 특별히 더 빠르게 실행하지 않음 |
    | 사용 목적 차이 | 람다는 **짧고 간단한 함수**에 적합, 일반 함수는 **복잡한 로직**에 적합 |
    
    🧩 그럼 언제 람다를 쓰는 게 좋을까?
    
    - **한 줄짜리 간단한 함수**를 임시로 쓸 때
    - `map`, `filter`, `sorted`, `key` 인자 등에 콜백 함수로 넘길 때
    
    하지만 복잡한 로직이 들어가거나 디버깅이 필요하면 **일반 함수**를 쓰는 게 훨씬 가독성이 좋아요.
    

---

## 객체지향/함수형 프로그래밍

> 객체(Object) 지향(Oriented) 프로그래밍(Programming)
> 

만들어 놓고 사용하는 것, 만들지 않은 것은 쓸 수 없음

- ( ↔ ) 절차 지향 프로그래밍
    - 한 줄씩 순서대로 코드를 작성해놓고, 순서에 맞게 실행시킴
    - 번 필요한 순서대로 코드 작성으로 코드의 길이/중복이 많아짐
- 함수형 프로그래밍
    - 정의역 / 치역 관계처럼 해줘 → 수행 (중간에 결과가 바뀔 일 X)
    - 만든 그 자리에서 동장을 반영시키는 종류의 문법

## List Comprehension

> Comprehension - 이해력, 포함, 내포
> 
- 기존 List를 사용해서 간단히 다른 리스트를 만드는 기법
- for + append 보다 속도가 빠릅니다.
    - for문은 실행할때마다 메모리 재할당이 발생
    - 대부분 프로그램이 느려지는 이유는 재할당 때문입니다.
    
    ```python
    %%time
    
    list1 = []
    for i in range(1, 1000000):
        list1.append(i)
    
    #list1
    
    # CPU times: user 90.3 ms, sys: 18.1 ms, total: 108 ms
    # Wall time: 109 ms
    ```
    
    ```python
    %%time
    # [(변수에 적용할 수식) for (변수) in (for문이 돌아가는 범위)]
    
    list2 = [i for i in range(1,1000000)]
    
    #list2
    
    # CPU times: user 34.8 ms, sys: 26.2 ms, total: 61 ms
    # Wall time: 64 ms
    ```
    

- 짝수 리스트 만들기 실습 : **for < list comp + if < list comp + range(,,2)**
    
    ```python
    list_even = []
    list_odd = []
    for i in range(1, 100000) :
        list_even.append(i) if i%2==0 else list_odd.append(i)
        
    CPU times: user 13.9 ms, sys: 2.01 ms, total: 15.9 ms
    Wall time: 15.8 ms
    ```
    
    ```python
    %%time
    
    list_even = [i for i in range(1, 100000) if i %2 ==0]
    
    CPU times: user 5.42 ms, sys: 946 µs, total: 6.37 ms
    Wall time: 6.28 ms
    ```
    
    ```python
    %%time
    
    list_even = [i for i in range(0, 100000, 2)]
    
    CPU times: user 1.48 ms, sys: 0 ns, total: 1.48 ms
    Wall time: 1.49 ms
    ```
    

## Map

---

- 시퀀스 자료형 각 요소에 동일한 함수를 적용할 때
- map 함수가 반환한 객체는 반복 가능한(iterable) 객체
- 실행시점에 값을 생성하기 때문에 메모리 효율적

**/*map 으로 요소 하나씩 꺼내기**

```python
li = [1,2,3,4,5]

def plus_one(x):
    return x+1

[map(plus_one, li)] #[<map at 0x7f2c2b0c26b0>]

[*map(plus_one, lim)] #[2, 3, 4, 5, 6]

[*map(lambda x: x+1, li)] #[2, 3, 4, 5, 6]
```

**대문자로 바꾸기**

```python
words = ['apple', '사과나무', '111']
[*map(str.upper, words)]

# result
['APPLE', '사과나무', '111'] # int는 upper 적용 불가
```

**형변환**

```python
# 이 문자열들을 실수로 바꾸고 싶어요!
string_floats = ['1.1', '2.2', '3.3', '4.4', '5.5']

[*map(float, string_floats)]
```

## Filter

> 이터러블의 각 요소에 대해 function이 참인 것만 반환하는 요소의 이터레이터
> 

↔ Map, 참이 아닌 값에 대해서는 None/False 값이 할당됨

## Enumerate / Zip

> **Enumerate(열거하다)**
: 리스트의 요소를 추출할 때 번호를 붙여서 추출해 줍니다
> 

> **Zip (잠그다)**
: 두 개의 리스트를 병렬적으로 추출함
> 

```python
alist = ['사과', '바나나', '딸기'] blist = ['Apple', 'Banana', 'Strawberry']
[print(idx, v0, v1) for idx, (v0, v1) in enumerate(zip(alist, blist))]

# result
0 사과 Apple
1 바나나 Banana
2 딸기 Strawberry
[None, None, None]
```

**Q. 왜 마지막에 [None, None, None] 이 붙는걸까?**

> 이 코드는 **리스트 컴프리헨션 안에서 `print()`를 실행**하고 있어요. 그런데 `print()` 함수는 **항상 `None`을 반환**합니다. 그래서 이 리스트 컴프리헨션은 다음과 같은 리스트를 생성하게 돼요: `[None, None, None]`
> 

## Reduce

- reduce는 반복 가능한 객체의 각 요소를 지정된 함수로 처리한 뒤 이전 결과와 **누적해서 반환**
- reduce는 파이썬 3부터 내장 함수가 아님
- functools 모듈에서 reduce 함수를 가져와야 함
- `reduce(function, iterable, initializer=None)`
- `from functools import reduce`

<aside>
⚠️

0부터 1000000 까지 더하는 연산을 했을 때, 어떤게 더 빠를까?

---

```python
reduce(lambda x,y:x+y, range(1000000))

# result
CPU times: user 84.7 ms, sys: 0 ns, total: 84.7 ms
Wall time: 84.7 ms
```

```python
reduce(operator.add, range(1000000))

CPU times: user 56 ms, sys: 0 ns, total: 56 ms
Wall time: 55.9 ms

# result
CPU times: user 56 ms, sys: 0 ns, total: 56 ms
Wall time: 55.9 ms
```

```python
sum([i for i in range(1000000)])

# result
CPU times: user 60.6 ms, sys: 0 ns, total: 60.6 ms
Wall time: 60.4 ms
```

```python
result = 0
for i in range(0, 1000000):
    result += i

result

# result
CPU times: user 228 ms, sys: 0 ns, total: 228 ms
Wall time: 242 ms
```

</aside>

- reduce 실습
    - 초기값은 list안에 dict가 들어있는 다차원 데이터구조에서 처음 값을 의미
    
    ```python
    reduce(lambda n, person: ( n + person['age']), test2, 0) / len(test2) 
    
    #reduce( 람다함수 매개변수1, 매개변수2 : 표현식, 리스트, 초기값)
    ```
    

## Generator

> 파이썬에서 yield 키워드를 사용하여 이터레이터(iterator)를 반환
→ 일반 함수와 달리, 한 번에 모든 값을 반환하지 않고 필요할 때마다 하나씩 값을 생성
> 
- **Lazy Evaluation (지연 계산)**
    
    값을 요청할 때마다 하나씩 계산하므로, 전체 데이터를 한꺼번에 메모리에 로드하지 않습니다. 예를 들어, 매우 큰 로그 파일을 처리할 때 유용합니다.
    
- **메모리 효율성**
    
    한 번에 하나의 값만 생성하므로, 큰 데이터셋을 처리할 때 메모리 사용량이 크게 줄어듭니다. (메모리 사용 최소화)
    
- **코드 가독성과 유지보수**
    
    복잡한 반복 로직을 단순화하여, 더 읽기 쉽고 관리하기 쉬운 코드를 작성할 수 있습니다.
    
- ex ) 파일을 한 줄씩 읽어야 할 때, 파일을 읽을 때에만 읽도록 만들면 메모리에 불필요한 값을 할당할 필요 없이, 실행 할때만 수행될 수 있다

 

- **Copilot, 제너레이터를 사용하는 예시 상황**
    
    좋은 질문이에요! **제너레이터(generator)**는 파이썬에서 **메모리를 아끼면서 데이터를 순차적으로 처리**할 수 있는 아주 강력한 도구예요. 특히 **데이터가 많거나, 한 번에 다 처리할 필요가 없을 때** 유용하죠.
    
    ---
    
    ## 🔄 제너레이터가 유용한 예시 상황들
    
    ### 1. 📁 대용량 파일 처리
    
    ```python
    def read_large_file(filename):
        with open(filename, 'r') as f:
            for line in f:
                yield line.strip()
    
    ```
    
    - 수백 MB짜리 파일도 한 줄씩 읽어서 처리 가능
    - `yield`를 쓰면 한 줄씩 메모리에 올리므로 **메모리 폭발 방지**
    
    ---
    
    ### 2. 🔢 무한 수열 생성
    
    ```python
    def infinite_counter(start=0):
        while True:
            yield start
            start += 1
    
    ```
    
    - 무한 루프처럼 보이지만, `next()`로 하나씩 꺼내 쓰면 안전
    - 예: 무한한 ID 발급기, 시뮬레이션 시간 흐름 등
    
    ---
    
    ### 3. 🧮 계산 비용이 큰 작업을 지연 실행
    
    ```python
    def expensive_computation(data):
        for x in data:
            yield x ** 2  # 예: 복잡한 수식 대신 간단한 제곱
    
    ```
    
    - 모든 결과를 한꺼번에 계산하지 않고, **필요할 때만 계산**
    - 예: 머신러닝 전처리, 실시간 데이터 분석
    
    ---
    
    ### 4. 🧵 파이프라인 처리 (map/filter처럼)
    
    ```python
    data = range(1000000)
    
    # 제너레이터 체이닝
    squares = (x**2 for x in data if x % 2 == 0)
    
    ```
    
    - 리스트 컴프리헨션 대신 **제너레이터 표현식**을 쓰면 메모리 절약
    - 중간 결과를 저장하지 않고 바로 다음 단계로 넘김
    
    ---
    
    ### 5. 🧪 테스트용 더미 데이터 생성
    
    ```python
    def fake_users():
        names = ['Alice', 'Bob', 'Charlie']
        for i in range(1000):
            yield {'id': i, 'name': names[i % len(names)]}
    
    ```
    
    - 테스트나 시뮬레이션용으로 **가짜 데이터**를 무한히 생성 가능
    
    ---
    
    ## ✅ 제너레이터를 써야 할 때 vs 리스트
    
    | 상황 | 제너레이터 추천 | 리스트 추천 |
    | --- | --- | --- |
    | 데이터가 매우 크다 | ✅ | ❌ |
    | 한 번만 순회하면 된다 | ✅ | ❌ |
    | 전체 데이터를 여러 번 써야 한다 | ❌ | ✅ |
    | 빠른 인덱싱이 필요하다 | ❌ | ✅ |
    | 실시간 처리 / 스트리밍 데이터 | ✅ | ❌ |
    
    ---
    
    필요하다면 `yield`와 `return`의 차이, 제너레이터와 이터레이터의 관계도 설명해드릴 수 있어요. 궁금하신가요? 😄
    

## 표준 출력